{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\diego\\tensorflow_datasets\\oxford_flowers102\\2.1.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:01<?, ? url/s]\n",
      "Dl Completed...:  33%|███▎      | 1/3 [00:01<00:03,  1.72s/ url]\n",
      "Dl Completed...:  33%|███▎      | 1/3 [00:01<00:03,  1.72s/ url]\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:01<00:01,  1.72s/ url]\n",
      "Extraction completed...: 100%|██████████| 8189/8189 [02:19<00:00, 58.59 file/s] \n",
      "Dl Size...: 100%|██████████| 328/328 [02:19<00:00,  2.35 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [02:19<00:00, 46.59s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset oxford_flowers102 downloaded and prepared to C:\\Users\\diego\\tensorflow_datasets\\oxford_flowers102\\2.1.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 4s/step - accuracy: 0.0353 - loss: 4.6035 - val_accuracy: 0.1691 - val_loss: 3.9751\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 3s/step - accuracy: 0.3598 - loss: 3.3596 - val_accuracy: 0.4064 - val_loss: 2.7836\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 4s/step - accuracy: 0.6332 - loss: 2.0132 - val_accuracy: 0.5786 - val_loss: 1.9955\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 3s/step - accuracy: 0.8447 - loss: 1.0275 - val_accuracy: 0.6357 - val_loss: 1.6345\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 4s/step - accuracy: 0.9397 - loss: 0.6086 - val_accuracy: 0.6673 - val_loss: 1.4361\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.9549 - loss: 0.4228 - val_accuracy: 0.6962 - val_loss: 1.2664\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 3s/step - accuracy: 0.9863 - loss: 0.2870 - val_accuracy: 0.7027 - val_loss: 1.2280\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3s/step - accuracy: 0.9914 - loss: 0.1889 - val_accuracy: 0.7144 - val_loss: 1.1742\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.9993 - loss: 0.1405 - val_accuracy: 0.7227 - val_loss: 1.1149\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 3s/step - accuracy: 0.9997 - loss: 0.1005 - val_accuracy: 0.7325 - val_loss: 1.0782\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Cargar el dataset Oxford Flowers 102\n",
    "dataset, info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True, split=['train', 'test'])\n",
    "\n",
    "# Preprocesar las imágenes: redimensionar y normalizar\n",
    "IMG_SIZE = 224\n",
    "def format_image(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) / 255.0 #Se normalizan los valores de los píxeles dividiéndolos por 255 \n",
    "                                                                 #para llevarlos al rango [0, 1], lo que ayuda a que el modelo entrene de manera más eficiente.\n",
    "    return image, label\n",
    "\n",
    "train_dataset = dataset[0].map(format_image)\n",
    "test_dataset = dataset[1].map(format_image)\n",
    "\n",
    "# Batching y shuffling (Para mejorar la eficiencia durante el entrenamiento, las imágenes se agrupan en batches de tamaño 32)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.shuffle(1024).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE) (#se aplica shuffling para asegurar que el modelo no aprenda patrones basados en el orden de las imágenes.)\n",
    "\n",
    "# Cargar la red preentrenada MobileNetV2 sin la capa superior\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Congelar las capas de la red preentrenada\n",
    "base_model.trainable = False\n",
    "\n",
    "# Añadir nuevas capas densas para la tarea de clasificación multiclase\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(102, activation='softmax')  # Clasificación multiclase (102 clases)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este caso utilizaremos Transfer Learning para clasificar imágenes del dataset Oxford Flowers 102, que contiene más de 7,000 imágenes de 102 especies diferentes de flores. La tarea consiste en predecir a qué categoría pertenece cada imagen de flor, lo cual es un reto debido a la variedad visual entre clases.\n",
    "\n",
    "Para abordar esta tarea, emplearemos el modelo preentrenado MobileNetV2, que ha sido entrenado previamente en el dataset ImageNet con millones de imágenes. Aprovecharemos las características visuales que este modelo ya ha aprendido, lo que nos permitirá mejorar la precisión y reducir el tiempo de entrenamiento.\n",
    "\n",
    "Se escogió el dataset Oxford Flowers 102 porque ofrece un desafío interesante para la clasificación de imágenes debido a su gran variedad visual con 102 categorías diferentes de flores. Esto lo hace ideal para aplicar Transfer Learning, ya que podemos aprovechar las características aprendidas por modelos preentrenados para mejorar la precisión en un conjunto de datos específico y complejo. Además, es un dataset bien etiquetado y reconocido en tareas de visión por computadora, lo que facilita la evaluación y comparación de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración, Explicación y Limpieza de Datos\n",
    "\n",
    "1. Explicación de dónde se obtuvo el dataset y su contexto:\n",
    "\n",
    "El dataset Oxford Flowers 102 se obtuvo a través de la librería TensorFlow Datasets. Fue creado por la Universidad de Oxford y contiene imágenes de 102 especies de flores. Este dataset es ampliamente utilizado en tareas de clasificación de imágenes multiclase debido a la gran variabilidad visual que presentan las flores, lo que lo convierte en un buen desafío para modelos de visión por computadora.\n",
    "\n",
    "El dataset tiene más de 7,000 imágenes, y las clases están desbalanceadas, es decir, algunas clases tienen más imágenes que otras. Esto plantea un reto adicional en el proceso de entrenamiento del modelo, pero puede manejarse adecuadamente con técnicas de Transfer Learning.\n",
    "\n",
    "2. Análisis del dataset:\n",
    "\n",
    "Número de instancias: El dataset contiene 7,189 imágenes en total, divididas en 102 clases.\n",
    "\n",
    "Número de columnas: Cada imagen está acompañada de una etiqueta (label) que indica a qué especie de flor pertenece, por lo que el dataset tiene dos columnas principales: la imagen (input) y la etiqueta (output).\n",
    "\n",
    "Tipos de datos:\n",
    "    Las imágenes son arrays de píxeles con 3 canales (RGB), que deben ser redimensionadas para que tengan un tamaño uniforme.\n",
    "    Las etiquetas son valores enteros que representan las 102 clases de flores. Estas etiquetas están codificadas numéricamente del 0 al 101."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados e Interpretación\n",
    "1. Resultados del modelo\n",
    "\n",
    "Después de entrenar el modelo durante 10 épocas utilizando Transfer Learning con MobileNetV2, los resultados son los siguientes:\n",
    "\n",
    "    Precisión en el conjunto de entrenamiento (Epoch 10): 99.97%\n",
    "    Pérdida en el conjunto de entrenamiento (Epoch 10): 0.1005\n",
    "    Precisión en el conjunto de validación (Epoch 10): 73.25%\n",
    "    Pérdida en el conjunto de validación (Epoch 10): 1.0782\n",
    "\n",
    "2. Interpretación de los resultados\n",
    "\n",
    "    Rendimiento en entrenamiento: El modelo alcanzó una precisión muy alta (99.97%) en el conjunto de entrenamiento, lo que indica que ha aprendido a clasificar correctamente las imágenes de flores con bastante exactitud.\n",
    "\n",
    "    Rendimiento en validación: La precisión en el conjunto de validación es más baja (73.25%), lo que es normal dado que el conjunto de validación contiene datos no vistos previamente. Esto indica que el modelo ha generalizado bien, aunque existe una diferencia considerable entre entrenamiento y validación. Esta diferencia podría ser señal de un posible sobreajuste.\n",
    "\n",
    "3. Análisis del sobreajuste\n",
    "\n",
    "    La alta precisión en el entrenamiento (casi el 100%) y la menor precisión en el conjunto de validación (73.25%) sugieren que el modelo podría estar sobreajustándose a los datos de entrenamiento.\n",
    "    Sin embargo, la mejora continua en la precisión de validación y la disminución constante de la pérdida sugieren que el modelo sigue aprendiendo, y más epochs de entrenamiento podrían mejorar el rendimiento en validación.\n",
    "\n",
    "4. Posibles mejoras\n",
    "\n",
    "    Descongelar algunas capas del modelo preentrenado: Para mejorar la precisión en el conjunto de validación, se podría intentar descongelar algunas de las capas superiores de MobileNetV2 para permitir un ajuste más fino de los pesos en las últimas capas convolucionales. Esto permitiría al modelo aprender características más específicas de las flores.\n",
    "\n",
    "    Aumentar el número de epochs: Continuar entrenando el modelo más allá de las 10 epochs puede mejorar la precisión en el conjunto de validación, ya que el modelo parece seguir mejorando con cada epoch.\n",
    "\n",
    "    Técnicas de regularización: Para mitigar el sobreajuste, se podrían añadir técnicas de regularización como Dropout o Early Stopping para detener el entrenamiento cuando la precisión de validación deje de mejorar.\n",
    "\n",
    "En resumen, los resultados muestran un buen rendimiento general del modelo, con alta precisión en el conjunto de entrenamiento y una precisión aceptable en el conjunto de validación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "En este notebook, aplicamos Transfer Learning con MobileNetV2 para clasificar imágenes del dataset Oxford Flowers 102 en 102 categorías. El modelo logró una alta precisión del 99.97% en el entrenamiento y un 73.25% en validación, mostrando buena generalización pero con indicios de sobreajuste.\n",
    "\n",
    "Transfer Learning permitió reducir el tiempo de entrenamiento y obtener buenos resultados. Para mejorar el rendimiento, sería útil descongelar capas adicionales y aplicar regularización. En general, el objetivo de clasificar imágenes de flores fue cumplido con éxito."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
